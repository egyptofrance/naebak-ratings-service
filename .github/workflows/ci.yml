name: Naibak Microservice CI Pipeline

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: 3.11
  MINIMUM_COVERAGE: 90

jobs:
  # Pre-flight checks
  pre-flight:
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.changes.outputs.python }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for Python changes
        uses: dorny/paths-filter@v2
        id: changes
        with:
          filters: |
            python:
              - '**/*.py'
              - 'requirements.txt'
              - 'pytest.ini'
              - '.pre-commit-config.yaml'

  # Code quality and security checks
  quality-and-security:
    runs-on: ubuntu-latest
    needs: pre-flight
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pre-commit

      - name: Cache pre-commit
        uses: actions/cache@v3
        with:
          path: ~/.cache/pre-commit
          key: pre-commit-${{ runner.os }}-${{ hashFiles('.pre-commit-config.yaml') }}

      - name: Run pre-commit hooks
        run: |
          pre-commit install
          pre-commit run --all-files --show-diff-on-failure

      - name: AI Governance Check
        run: |
          echo "ü§ñ Running AI Governance checks..."
          python scripts/ai_governance_hook.py $(find app/ -name "*.py" -type f)
        continue-on-error: false

      - name: Security scan with Bandit
        run: |
          echo "üîí Running security scan..."
          bandit -r app/ -f json -o bandit-report.json
          bandit -r app/ -ll
          # Fail if any medium or high severity issues found
          if [ -f bandit-report.json ]; then
            ISSUES=$(jq '.results | length' bandit-report.json)
            if [ "$ISSUES" -gt 0 ]; then
              echo "‚ùå Security vulnerabilities found!"
              jq '.results[] | {filename: .filename, issue_text: .issue_text, issue_severity: .issue_severity}' bandit-report.json
              exit 1
            fi
          fi

      - name: Dependency security check
        run: |
          echo "üì¶ Checking dependencies for vulnerabilities..."
          pip install safety
          safety check --json --output safety-report.json
          safety check

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json

  # Comprehensive testing
  test:
    runs-on: ubuntu-latest
    needs: [pre-flight, quality-and-security]
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_naibak
          POSTGRES_USER: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Wait for services
        run: |
          echo "‚è≥ Waiting for services to be ready..."
          sleep 10

      - name: Run database migrations
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_naibak
          REDIS_URL: redis://localhost:6379/0
          SECRET_KEY: test-secret-key-for-ci
          DEBUG: True
        run: |
          python manage.py migrate --settings=config.settings

      - name: Code-Test Ratio Check
        run: |
          echo "üìä Checking code-to-test ratio..."
          python scripts/code_test_ratio_check.py

      - name: Run unit tests with coverage
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_naibak
          REDIS_URL: redis://localhost:6379/0
          SECRET_KEY: test-secret-key-for-ci
          DEBUG: True
        run: |
          echo "üß™ Running unit tests..."
          pytest tests/unit/ -v \
            --cov=app \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --cov-fail-under=${{ env.MINIMUM_COVERAGE }} \
            --maxfail=5 \
            --tb=short

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_naibak
          REDIS_URL: redis://localhost:6379/0
          SECRET_KEY: test-secret-key-for-ci
          DEBUG: True
        run: |
          echo "üîó Running integration tests..."
          pytest tests/integration/ -v --maxfail=3

      - name: Run AI Governance tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_naibak
          REDIS_URL: redis://localhost:6379/0
          SECRET_KEY: test-secret-key-for-ci
          DEBUG: True
        run: |
          echo "ü§ñ Running AI Governance tests..."
          pytest tests/governance/ -v --maxfail=1

      - name: Run security tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_naibak
          REDIS_URL: redis://localhost:6379/0
          SECRET_KEY: test-secret-key-for-ci
          DEBUG: True
        run: |
          echo "üîí Running security tests..."
          pytest tests/security/ -v --maxfail=1

      - name: Coverage Quality Check
        run: |
          echo "üìà Running comprehensive coverage check..."
          python scripts/coverage_check.py

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: true

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results
          path: |
            coverage.xml
            htmlcov/
            pytest-report.xml

  # Contract testing
  contract-testing:
    runs-on: ubuntu-latest
    needs: [test]
    if: needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install schemathesis

      - name: Contract validation
        run: |
          echo "üìã Running contract validation..."
          python scripts/contract_validation.py

      - name: API schema validation
        run: |
          echo "üîç Validating API schemas..."
          if [ -f "docs/openapi.yaml" ]; then
            echo "OpenAPI schema found, running validation..."
            # Add OpenAPI validation here
          else
            echo "‚ö†Ô∏è No OpenAPI schema found, skipping validation"
          fi

  # Performance testing (optional, runs on main branch)
  performance:
    runs-on: ubuntu-latest
    needs: [contract-testing]
    if: github.ref == 'refs/heads/main' && needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install locust

      - name: Run performance tests
        run: |
          echo "‚ö° Running performance tests..."
          if [ -d "tests/performance" ]; then
            pytest tests/performance/ -v
          else
            echo "‚ö†Ô∏è No performance tests found, skipping"
          fi

  # Build Docker image (only on main/develop)
  build:
    runs-on: ubuntu-latest
    needs: [contract-testing]
    if: (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop') && needs.pre-flight.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: false
          tags: naibak-ratings-service:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Test Docker image
        run: |
          echo "üê≥ Testing Docker image..."
          docker run --rm naibak-ratings-service:${{ github.sha }} python manage.py check

  # Notification and summary
  notify:
    runs-on: ubuntu-latest
    needs: [quality-and-security, test, contract-testing, build]
    if: always()
    steps:
      - name: Check job results
        run: |
          echo "üìä Pipeline Summary:"
          echo "Quality & Security: ${{ needs.quality-and-security.result }}"
          echo "Tests: ${{ needs.test.result }}"
          echo "Contract Testing: ${{ needs.contract-testing.result }}"
          echo "Build: ${{ needs.build.result }}"

      - name: Success notification
        if: ${{ needs.quality-and-security.result == 'success' && needs.test.result == 'success' && needs.contract-testing.result == 'success' }}
        run: |
          echo "‚úÖ All checks passed! Code is ready for deployment."

      - name: Failure notification
        if: ${{ needs.quality-and-security.result == 'failure' || needs.test.result == 'failure' || needs.contract-testing.result == 'failure' }}
        run: |
          echo "‚ùå Some checks failed. Please review the logs and fix the issues."
          exit 1
